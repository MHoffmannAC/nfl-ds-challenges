{"cells":[{"cell_type":"markdown","metadata":{"id":"oV-E3sCPa2-K"},"source":["# Data Science Workshop: Web Scraping & MySQL Integration\n","\n","Welcome, Data Science students! In this workshop, we'll dive into practical data acquisition techniques: web scraping and interacting with databases. Our goal is to gather information about NFL teams, their conferences, divisions, personnel, and championships, store it in a structured way, and then persist it in a MySQL database.\n","\n","## Learning Objectives:\n","1.  **Web Scraping:** Extract structured data from websites using `requests` and `BeautifulSoup`.\n","2.  **Data Manipulation:** Organize scraped data into Pandas DataFrames.\n","3.  **Function Design:** Create reusable functions for data extraction and updates.\n","4.  **Database Interaction:** Understand SQL schema, create tables, and push/pull data using Python's `sqlalchemy` and Pandas `to_sql`/`read_sql`.\n","5.  **Data Integration:** Link related data across tables using primary and foreign keys.\n","\n","**Important Note:** If you encounter issues, please ensure your `keys.py` file is correctly configured with your MySQL password and that your MySQL server is running before executing the database connection and table creation steps."]},{"cell_type":"markdown","metadata":{"id":"_g0jmQBpa2-N"},"source":["## Part 1: Setting Up Environment\n","\n","First, we'll import the necessary libraries required for web scraping, data manipulation, and database interaction."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x6dcWmkda2-O"},"outputs":[],"source":["import requests\n","from bs4 import BeautifulSoup\n","import pandas as pd\n","from datetime import datetime\n","import re\n","\n","from sqlalchemy import create_engine, text\n","from sqlalchemy.exc import OperationalError, ProgrammingError\n","\n","import keys"]},{"cell_type":"markdown","metadata":{"id":"VJstr6SOa2-P"},"source":["## Part 2: Implementing Helper Functions\n","\n","In this section, you will implement several helper functions. These functions will be crucial for abstracting the logic for web scraping specific types of data and for pushing DataFrames to our MySQL database. Building these functions step-by-step will prepare you for the main data pipeline."]},{"cell_type":"markdown","metadata":{"id":"ng18jljYa2-Q"},"source":["### Task 2.1: Implement `get_nfl_teams_from_wiki`\n","\n","Create a Python function named `get_nfl_teams_from_wiki`. This function should:\n","1.  Define the URL for the National Football League Wikipedia page.\n","2.  Fetch the HTML content using `requests` and parse it with `BeautifulSoup`.\n","3.  Locate the table containing NFL team information. This table can often be identified by its caption (e.g., \"National Football League teams\") or by checking for specific classes and headers within candidate tables.\n","4.  Parse the table, handling `rowspan` and `colspan` attributes to correctly reconstruct the grid of data.\n","5.  Extract the 'conference', 'division', and 'team_name' names from the parsed table.\n","6.  Clean the extracted team names (e.g., remove footnote symbols like `[*]`, `[†]`, and text within square brackets `[1]`).\n","7.  Return a Pandas DataFrame with columns 'conference', 'division', and 'team_name'. This DataFrame will serve as the source for both the `teams` and `conferences` tables in our database."]},{"cell_type":"markdown","metadata":{"id":"UjSk6sVGa2-R"},"source":["##### Hint\n","<details>\n","<summary>Click to reveal hint</summary>\n","\n","Here's a step-by-step guide to help you implement `get_nfl_teams_from_wiki`:\n","\n","**Step 1: Define URL and Fetch Content**\n","* Set `url` to `\"https://en.wikipedia.org/wiki/National_Football_League\"`.\n","* Use `requests.get(url)` and `response.raise_for_status()` for error handling.\n","* Parse the content with `BeautifulSoup(response.text, 'html.parser')`.\n","\n","**Step 2: Locate the NFL Teams Table**\n","* Try to find the table by its `<caption>` tag, looking for text like \"National Football League teams\" (use `re.compile` with `re.IGNORECASE` for flexibility).\n","\n","**Step 3: Handle `rowspan` and `colspan` to Build a Grid**\n","* Initialize a 2D list (grid) to represent the table, filled with `None`. The number of columns can be determined by summing `colspan` values from the first header row.\n","* Iterate through each `<tr>` (HTML row) in the `nfl_teams_table`.\n","* For each `<th>` or `<td>` (HTML cell) in the current `<tr>`:\n","    * Determine its `rowspan` and `colspan` attributes (default to 1 if not present).\n","    * Find the next available `c_idx` (column index) in your `grid` for the current `r_idx` (row index), skipping cells already filled by a previous span.\n","    * Fill the corresponding cells in your `grid` with the `cell_value` for the extent of its `rowspan` and `colspan`.\n","    * Advance `c_idx` by the `colspan` value.\n","\n","**Step 4: Extract Data from the Grid**\n","* The actual data rows usually start after the header rows (e.g., from `grid[1]` onwards).\n","* For each data row in your `grid`:\n","    * 'Conference' is at `row_values[0]`.\n","    * 'Division' is at `row_values[1]`.\n","    * 'Team' name position is at `row_values[2]`\n","* **Clean Team Names:** Use `re.sub(r'\\[.*?\\]|\\*|\\†', '', team_name).strip()` to remove footnotes and other unwanted characters.\n","\n","**Step 5: Return DataFrame**\n","* Append dictionaries containing 'Conference', 'Division', and 'Team' to a list.\n","* Convert this list of dictionaries into a Pandas DataFrame.\n","</details>"]},{"cell_type":"markdown","metadata":{"id":"XnXnG6Jha2-S"},"source":["##### Solution"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PUKNqYz_a2-S"},"outputs":[],"source":["def get_nfl_teams_from_wiki():\n","    \"\"\"\n","    Scrapes the NFL teams table from Wikipedia to extract Conference, Division, and Team names.\n","    Returns a Pandas DataFrame with 'conference', 'division', and 'team_name' columns.\n","    \"\"\"\n","    url = \"https://en.wikipedia.org/wiki/National_Football_League\"\n","    teams_final_data = []\n","\n","    try:\n","        response = requests.get(url)\n","        response.raise_for_status()\n","        soup = BeautifulSoup(response.text, 'html.parser')\n","\n","        caption = soup.find('caption', string=re.compile(r'National Football League teams', re.IGNORECASE))\n","        nfl_teams_table = caption.find_parent('table')\n","\n","        if nfl_teams_table is None:\n","            print(\"Error: Could not find the NFL teams table on the page.\")\n","            return pd.DataFrame()\n","        else:\n","            table_rows = nfl_teams_table.find_all('tr')\n","\n","            # Determine the number of columns by summing colspan values from the first header row\n","            num_cols = sum(int(th.get('colspan', 1)) for th in table_rows[0].find_all('th'))\n","\n","            # Initialize a grid to handle rowspan and colspan\n","            grid = [[None for _ in range(num_cols)] for _ in range(len(table_rows))]\n","\n","            for r_idx, row_html in enumerate(table_rows):\n","                c_idx = 0\n","\n","                for cell_html in row_html.find_all(['th', 'td']):\n","                    # Skip cells already filled by a previous span\n","                    while c_idx < num_cols and grid[r_idx][c_idx] is not None:\n","                        c_idx += 1\n","\n","                    if c_idx >= num_cols:\n","                        break\n","\n","                    rowspan = int(cell_html.get('rowspan', 1))\n","                    colspan = int(cell_html.get('colspan', 1))\n","                    cell_value = re.sub(r'\\s+', ' ', cell_html.get_text(strip=True)).strip()\n","\n","                    # Fill the grid with the cell value for the extent of its rowspan and colspan\n","                    for i in range(r_idx, r_idx + rowspan):\n","                        for j in range(c_idx, c_idx + colspan):\n","                            if i < len(grid) and j < num_cols:\n","                                grid[i][j] = cell_value\n","\n","                    c_idx += colspan\n","\n","            # Extract data from the grid, skipping header rows (assuming data starts from row 1)\n","            for r_idx in range(1, len(grid)):\n","                row_values = grid[r_idx]\n","\n","                # Assign values based on expected column positions\n","                conference = row_values[0]\n","                division = row_values[1]\n","                team = row_values[2]\n","\n","                # Clean team name\n","                if team:\n","                    team = re.sub(r'\\[.*?\\]|\\*|\\†', '', team).strip()\n","\n","                teams_final_data.append({\n","                    'conference': conference,\n","                    'division': division,\n","                    'team_name': team\n","                })\n","\n","            df = pd.DataFrame(teams_final_data)\n","            return df\n","\n","    except requests.exceptions.RequestException as e:\n","        print(f\"Error fetching the URL: {e}\")\n","        return pd.DataFrame()\n","    except Exception as e:\n","        print(f\"An unexpected error occurred: {e}\")\n","        return pd.DataFrame()"]},{"cell_type":"markdown","metadata":{"id":"BBI6FxaTa2-T"},"source":["### Task 2.2: Implement `get_static_team_data`\n","\n","Create a Python function named `get_static_team_data` that takes a `team_name` as input. This function should:\n","1.  Construct the Wikipedia URL for the given team.\n","2.  Use `requests` to fetch the page content and `BeautifulSoup` to parse it.\n","3.  Extract the 'Location', 'Stadium', 'Established', 'Colors', 'Mascot', and 'Website' from the team's infobox.\n","4.  Clean the extracted text (e.g., remove `[\\d+]` references).\n","5.  Return a dictionary containing the scraped information, with keys directly matching the column names in the `teams` SQL table."]},{"cell_type":"markdown","metadata":{"id":"Gl9bn76Xa2-U"},"source":["##### Hint\n","<details>\n","<summary>Click to reveal hint</summary>\n","\n","Here's a step-by-step guide to help you implement `get_static_team_data`:\n","\n","**Step 1: Construct the URL and Fetch Content**\n","* Start by replacing spaces in `team_name` with underscores to form the Wikipedia URL (e.g., \"New England Patriots\" becomes \"New_England_Patriots\").\n","* Use `requests.get()` to fetch the HTML content of the Wikipedia page.\n","* Parse the HTML content using `BeautifulSoup(response.text, 'html.parser')`.\n","\n","**Step 2: Locate the Infobox**\n","* Most Wikipedia pages for entities like sports teams have a main information box on the right side. This is typically a `<table>` element with the class `infobox`.\n","* Use `soup.find_all('table', class_='infobox')` to find all infobox tables. You'll need to iterate through these to find the *main* one, which often contains a `<th>` (table header) with text like 'Basic info' or 'Personnel'.\n","\n","**Step 3: Extract Data from Infobox Rows**\n","* Once you've identified the correct `infobox` table, iterate through its `<tr>` (table row) elements.\n","* Within each `<tr>`, look for `<th>` (header) and `<td>` (data) tags. The `<th>` usually contains the label (e.g., 'Location', 'Stadium'), and the `<td>` contains the corresponding value.\n","* Use `.get_text(strip=True)` to extract clean text from these tags.\n","\n","**Step 4: Clean and Store Specific Data Points with appropriate Keys**\n","* **General Cleaning:** For all extracted `data_text`, use `re.sub(r'\\[\\d+\\]', '', data_text)` to remove citation numbers like `[1]`, `[2]`, etc.\n","* **'Established':** This field often contains a full date and age (e.g., \"August 14, 1959; 65 years ago\"). Use `re.search(r'\\b\\d{4}\\b', data_text)` to extract just the four-digit year and convert it to an integer. Store this in the dictionary with the key `'established_year'`.\n","* **'Stadium' and 'Location':** These can be tricky. The `<td>` might contain both the stadium name and location, sometimes linked. Try to get the text of the first `<a>` tag within the `<td>` for the stadium. For location, you might need to extract text that follows the stadium name, or look for other `<a>` tags that represent cities/states. Clean up any extra parentheses or newline characters. Store these with keys `'stadium'` and `'city'`/`'state'` respectively.\n","* **'Colors':** This might be a simple text string or contain color swatches. Try to get the direct text content using `data.find(text=True, recursive=False).strip()`. If that's empty, you might need to combine text from child `<a>` tags, stopping if you encounter a `<br>` tag. Store this with the key `'colors'`.\n","* **'Mascot' and 'Website':** These are usually straightforward text extractions. For 'Website', look for an `<a>` tag with `class_='external text'` and extract its `href` attribute. Store these with keys `'mascot'` and `'website'`.\n","* **`team_name`:** Ensure you include the original `team_name` input as `'team_name'` in the dictionary.\n","\n","**Step 5: Return the Dictionary**\n","* Populate your `static_info` dictionary with the extracted and cleaned values, ensuring all keys match your SQL schema.\n","</details>"]},{"cell_type":"markdown","metadata":{"id":"7X5javnna2-V"},"source":["##### Solution"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rFLuXf1Ia2-V"},"outputs":[],"source":["def get_static_team_data(team_name):\n","    \"\"\"\n","    Scrapes static information (City, State, Stadium, Established, Colors, Mascot, Website)\n","    for a given NFL team from its Wikipedia page.\n","    \"\"\"\n","    url_team_name = team_name.replace(' ', '_')\n","    url = f\"https://en.wikipedia.org/wiki/{url_team_name}\"\n","\n","    static_info = {\n","        'team_name': team_name,\n","        'city': None,\n","        'state': None,\n","        'stadium': None,\n","        'established_year': None,\n","        'colors': None,\n","        'mascot': None,\n","        'website': None\n","    }\n","\n","    try:\n","        response = requests.get(url)\n","        response.raise_for_status()\n","        soup = BeautifulSoup(response.text, 'html.parser')\n","    except requests.exceptions.RequestException as e:\n","        print(f\"Error fetching {url}: {e}\")\n","        return static_info\n","\n","    infobox = None\n","    infoboxes = soup.find_all('table', class_='infobox')\n","    for ibox in infoboxes:\n","        # Look for infoboxes that contain 'Basic info' or 'Personnel' headers\n","        if ibox.find('th', class_='infobox-header', string='Basic info') or \\\n","           ibox.find('th', class_='infobox-header', string='Personnel'):\n","            infobox = ibox\n","            break\n","\n","    if not infobox:\n","        print(f\"Could not find infobox for {team_name}\")\n","        return static_info\n","\n","    for row in infobox.find_all('tr'):\n","        header = row.find('th')\n","        data = row.find('td')\n","        if header and data:\n","            header_text = header.get_text(strip=True)\n","            data_text_cleaned = re.sub(r'\\[\\d+\\]', '', data.get_text(strip=True))\n","\n","            if 'Established' in header_text:\n","                match = re.search(r'\\b\\d{4}\\b', data_text_cleaned)\n","                if match:\n","                    static_info['established_year'] = int(match.group(0))\n","            elif 'Stadium' in header_text:\n","                stadium_link = data.find('a')\n","                if stadium_link:\n","                    static_info['stadium'] = stadium_link.get_text(strip=True)\n","\n","                full_td_text = data.get_text(strip=True)\n","                cleaned_td_text = re.sub(r'\\[\\d+\\]', '', full_td_text).strip()\n","\n","                if static_info['stadium'] and static_info['stadium'] in cleaned_td_text:\n","                    cleaned_td_text = cleaned_td_text.replace(static_info['stadium'], '', 1).strip()\n","\n","                parentheses_match = re.search(r'\\(([^)]*)\\)', cleaned_td_text)\n","                if parentheses_match:\n","                    location_in_paren = parentheses_match.group(1).strip()\n","                    parts = [p.strip() for p in location_in_paren.split(',') if p.strip()]\n","                    if len(parts) >= 1:\n","                        static_info['city'] = parts[0]\n","                    if len(parts) >= 2:\n","                        static_info['state'] = parts[1]\n","                else:\n","                    parts = [p.strip() for p in cleaned_td_text.split(',') if p.strip()]\n","                    if len(parts) >= 1:\n","                        static_info['city'] = parts[0]\n","                    if len(parts) >= 2:\n","                        static_info['state'] = parts[1]\n","\n","            elif 'Colors' in header_text:\n","                colors_text = None\n","                plainlist_div = data.find('div', class_='plainlist')\n","                if plainlist_div:\n","                    first_li = plainlist_div.find('li')\n","                    if first_li:\n","                        colors_text = re.sub(r'\\[\\d+\\]', '', first_li.get_text(strip=True)).strip()\n","\n","                colors_parts = []\n","                for content in data.contents:\n","                    if isinstance(content, str) and content.strip():\n","                        cleaned_part = re.sub(r'\\[\\d+\\]', '', content.strip())\n","                        colors_parts.append(cleaned_part)\n","                    elif content.name == 'br': # Stop at line breaks\n","                        break\n","                    elif content.name == 'style' or (content.name == 'span' and 'legend-color' in content.get('class', [])):\n","                        # Skip style tags or color legend spans\n","                        break\n","                    elif content.name == 'sup': # Skip superscript (citations)\n","                        continue\n","                    elif content.name == 'a' and content.get_text(strip=True) and content.get_text(strip=True).lower() not in ['colors', 'color']:\n","                        colors_parts.append(content.get_text(strip=True))\n","\n","                colors_text = ', '.join(filter(None, colors_parts)).strip()\n","                colors_text = colors_text.strip(', ') # Remove trailing comma if any\n","\n","                static_info['colors'] = colors_text if colors_text else None\n","\n","            elif 'Mascot' in header_text:\n","                static_info['mascot'] = data_text_cleaned\n","            elif 'Website' in header_text:\n","                website_link = data.find('a', class_='external text')\n","                if website_link:\n","                    static_info['website'] = website_link.get('href')\n","\n","    return static_info\n"]},{"cell_type":"markdown","metadata":{"id":"T7C9_3Z_a2-X"},"source":["### Task 2.3: Implement `get_personnel_data`\n","\n","Create a Python function named `get_personnel_data` that takes a `team_name` as input. This function should:\n","1.  Construct the Wikipedia URL for the given team.\n","2.  Scrape the 'Owner(s)', 'President', 'General manager', and 'Head coach' from the infobox.\n","3.  Add an `extraction_date` column with the current date.\n","4.  Return a dictionary with the extracted personnel data, with keys directly matching the column names in the `personnel` SQL table."]},{"cell_type":"markdown","metadata":{"id":"y7sqel5Ka2-Y"},"source":["##### Hint\n","<details>\n","<summary>Click to reveal hint</summary>\n","\n","Here's a step-by-step guide to help you implement `get_personnel_data`:\n","\n","**Step 1: Construct URL and Fetch Content**\n","* Similar to Task 2.1, form the Wikipedia URL and use `requests` and `BeautifulSoup` to get the parsed HTML.\n","\n","**Step 2: Locate the 'Personnel' Infobox**\n","* Again, search for `<table>` elements with `class='infobox'`.\n","* This time, specifically look for the infobox that contains a `<th>` with the text 'Personnel'. This indicates you've found the correct section for personnel information.\n","\n","**Step 3: Extract Personnel Details with appropriate Keys**\n","* Iterate through the `<tr>` elements within the identified 'Personnel' infobox.\n","* For each row, extract the text from the `<th>` (header) and `<td>` (data) tags.\n","* **Headers to look for:** 'Owner(s)', 'President', 'General manager', 'Head coach'. Be aware that 'General manager' might sometimes appear as 'GM'.\n","* **Clean Text:** Remember to use `re.sub(r'\\[\\d+\\]', '', data_text)` to remove any citation numbers from the extracted data.\n","* Store the extracted values in your `personnel_info` dictionary with keys matching your SQL schema: `'owner'`, `'president'`, `'general_manager'`, `'head_coach'`.\n","\n","**Step 4: Add `extraction_date`**\n","* Include a key-value pair in your `personnel_info` dictionary for `'extraction_date'`, setting its value to `datetime.now().strftime('%Y-%m-%d')`.\n","\n","**Step 5: Return the Dictionary**\n","* Return the `personnel_info` dictionary with all the extracted details.\n","</details>"]},{"cell_type":"markdown","metadata":{"id":"kdhP8W8Xa2-Y"},"source":["##### Solution"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2YdmPDaKa2-Z"},"outputs":[],"source":["def get_personnel_data(team_name):\n","    \"\"\"\n","    Scrapes the Wikipedia page for a given NFL team to extract personnel information\n","    (Owner, President, Head Coach, General Manager) and adds the current extraction date.\n","    \"\"\"\n","    url_team_name = team_name.replace(' ', '_')\n","    url = f\"https://en.wikipedia.org/wiki/{url_team_name}\"\n","\n","    personnel_info = {\n","        'team_name': team_name,\n","        'owner': None,\n","        'president': None,\n","        'head_coach': None,\n","        'general_manager': None,\n","        'extraction_date': datetime.now().strftime('%Y-%m-%d')\n","    }\n","\n","    try:\n","        response = requests.get(url)\n","        response.raise_for_status()\n","        soup = BeautifulSoup(response.text, 'html.parser')\n","    except requests.exceptions.RequestException as e:\n","        print(f\"Error fetching {url}: {e}\")\n","        return personnel_info\n","\n","    infobox = None\n","    infoboxes = soup.find_all('table', class_='infobox')\n","    for ibox in infoboxes:\n","        if ibox.find('th', class_='infobox-header', string='Personnel'):\n","            infobox = ibox\n","            break\n","\n","    if not infobox:\n","        print(f\"Could not find personnel infobox for {team_name}\")\n","        return personnel_info\n","\n","    for row in infobox.find_all('tr'):\n","        header = row.find('th')\n","        data = row.find('td')\n","        if header and data:\n","            header_text = header.get_text(strip=True)\n","            data_text = re.sub(r'\\[\\d+\\]', '', data.get_text(strip=True))\n","\n","            if 'Owner(s)' in header_text:\n","                personnel_info['owner'] = data_text\n","            elif 'President' in header_text:\n","                personnel_info['president'] = data_text\n","            elif 'Head coach' in header_text:\n","                personnel_info['head_coach'] = data_text\n","            elif 'General manager' in header_text or 'GM' in header_text:\n","                personnel_info['general_manager'] = data_text\n","\n","    return personnel_info\n"]},{"cell_type":"markdown","metadata":{"id":"PGYYyaPba2-Z"},"source":["### Task 2.4: Implement `get_championship_data`\n","\n","Create a Python function named `get_championship_data` that takes a Pandas DataFrame (`championships_df`) and a SQLAlchemy engine (`sql_engine`) as input. This function should:\n","1.  Construct the Wikipedia URL for the given team.\n","2.  Scrape the counts for 'League championships', 'Conference championships', and 'Division championships' from the infobox.\n","3.  Return a dictionary with the extracted championship data, with keys directly matching the column names in the `championships` SQL table. **Note: This function will NOT include `extraction_year` as the database will only store the latest championship data per team.**"]},{"cell_type":"markdown","metadata":{"id":"tlY1npMLa2-Z"},"source":["##### Hint\n","<details>\n","<summary>Click to reveal hint</summary>\n","\n","Here's a step-by-step guide to help you implement `get_championship_data`:\n","\n","**Step 1: Construct URL and Fetch Content**\n","* As before, create the Wikipedia URL and fetch/parse the HTML.\n","\n","**Step 2: Locate the 'Championships' Infobox Section**\n","* Find the `infobox` table that contains a `<th>` with the text 'Championships'. This section typically lists the counts for various championship types.\n","\n","**Step 3: Extract Championship Counts with appropriate Keys**\n","* The championship details might not be in direct `<td>` siblings of the 'Championships' header. You might need to find the parent `<tr>` of the 'Championships' header and then iterate through subsequent sibling `<tr>` elements.\n","* Within each relevant row, use `re.search()` with patterns like `r'League championships:\\s*(\\d+)'`, `r'Conference championships:\\s*(\\d+)'`, and `r'Division championships:\\s*(\\d+)'` to find the numerical counts.\n","* Remember to convert the extracted digits (which will be strings) to integers using `int()`.\n","* Note that 'League championships' might sometimes be represented by 'Super Bowl championships' on some pages. You can use the Super Bowl count as a fallback if the general 'League championships' isn't found.\n","* Store the extracted counts in your `championship_info` dictionary with keys matching your SQL schema: `'league_championships'`, `'conference_championships'`, `'division_championships'`.\n","\n","**Step 4: Return the Dictionary**\n","* Return the `championship_info` dictionary. **Crucially, do NOT include `extraction_year` in this dictionary, as the database will only store the latest championship data per team.**\n","</details>"]},{"cell_type":"markdown","metadata":{"id":"CZcgK2xea2-a"},"source":["##### Solution"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"of-Kw5-Ca2-a"},"outputs":[],"source":["def get_championship_data(team_name):\n","    \"\"\"\n","    Scrapes championship information (League, Conference, Division championships)\n","    for a given NFL team from its Wikipedia page.\n","    \"\"\"\n","    url_team_name = team_name.replace(' ', '_')\n","    url = f\"https://en.wikipedia.org/wiki/{url_team_name}\"\n","\n","    championship_info = {\n","        'team_name': team_name,\n","        'league_championships': None,\n","        'conference_championships': None,\n","        'division_championships': None,\n","    }\n","\n","    try:\n","        response = requests.get(url)\n","        response.raise_for_status()\n","        soup = BeautifulSoup(response.text, 'html.parser')\n","    except requests.exceptions.RequestException as e:\n","        print(f\"Error fetching {url}: {e}\")\n","        return championship_info\n","\n","    infobox = None\n","    infoboxes = soup.find_all('table', class_='infobox')\n","    for ibox in infoboxes:\n","        if ibox.find('th', class_='infobox-header', string='Championships'):\n","            infobox = ibox\n","            break\n","\n","    if not infobox:\n","        return championship_info\n","\n","    championships_header = infobox.find('th', class_='infobox-header', string='Championships')\n","    if championships_header:\n","        current_row = championships_header.find_parent('tr').find_next_sibling('tr')\n","        while current_row:\n","            row_text = current_row.get_text(strip=True)\n","\n","            league_match = re.search(r'League championships:\\s*(\\d+)', row_text)\n","            if league_match:\n","                championship_info['league_championships'] = int(league_match.group(1))\n","            elif 'Super Bowl championships' in row_text:\n","                super_bowl_match = re.search(r'Super Bowl championships:\\s*(\\d+)', row_text)\n","                if super_bowl_match and championship_info['league_championships'] is None:\n","                    championship_info['league_championships'] = int(super_bowl_match.group(1))\n","\n","            conference_match = re.search(r'Conference championships:\\s*(\\d+)', row_text)\n","            if conference_match:\n","                championship_info['conference_championships'] = int(conference_match.group(1))\n","\n","            division_match = re.search(r'Division championships:\\s*(\\d+)', row_text)\n","            if division_match:\n","                championship_info['division_championships'] = int(division_match.group(1))\n","\n","            next_row_candidate = current_row.find_next_sibling('tr')\n","            if next_row_candidate and next_row_candidate.find('th', class_='infobox-header'):\n","                break\n","            current_row = next_row_candidate\n","\n","    return championship_info\n"]},{"cell_type":"markdown","metadata":{"id":"WSgzAHfNa2-b"},"source":["### Task 2.5: Implement `push_teams_to_sql`\n","\n","Create a Python function named `push_teams_to_sql` that takes a Pandas DataFrame (`df`) and a SQLAlchemy engine (`sql_engine`) as input. This function should:\n","1.  Push the DataFrame to the `teams` table in the MySQL database.\n","2.  Handle potential duplicate team names by updating existing records or inserting new ones using `INSERT ... ON DUPLICATE KEY UPDATE`."]},{"cell_type":"markdown","metadata":{"id":"q9MfxaUxa2-b"},"source":["##### Hint\n","<details>\n","<summary>Click to reveal hint</summary>\n","\n","Here's a step-by-step guide to help you implement `push_teams_to_sql` using `INSERT ... ON DUPLICATE KEY UPDATE`:\n","\n","**Step 1: Iterate and Upsert**\n","* Iterate through each row of the input DataFrame `df`.\n","* For each row, construct an `INSERT` statement that includes an `ON DUPLICATE KEY UPDATE` clause.\n","* The `ON DUPLICATE KEY UPDATE` clause should specify which columns to update if a record with the same `team_name` (which is a `UNIQUE` key) already exists. Use `VALUES(column_name)` to refer to the new values being inserted.\n","* Execute this SQL statement using `connection.execute(text(...))` within a database connection.\n","\n","**Step 2: Commit Changes**\n","* After iterating through all rows, commit the changes to the database using `connection.commit()`.\n","</details>"]},{"cell_type":"markdown","metadata":{"id":"MVjC0fh2a2-c"},"source":["##### Solution"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-9ggJKGTa2-c"},"outputs":[],"source":["def push_teams_to_sql(df, sql_engine):\n","    \"\"\"\n","    Pushes the NFL teams DataFrame to the 'teams' table.\n","    Handles potential duplicates by updating existing team records or inserting new ones.\n","    \"\"\"\n","    if df.empty:\n","        print(\"No team data to push to 'teams' table.\")\n","        return\n","\n","    try:\n","        with sql_engine.connect() as connection:\n","            for index, row in df.iterrows():\n","                team_name = row['team_name']\n","                city = row.get('city')\n","                state = row.get('state')\n","                stadium = row.get('stadium')\n","                established_year = row.get('established_year')\n","                colors = row.get('colors')\n","                mascot = row.get('mascot')\n","                website = row.get('website')\n","\n","                sql_statement = text(\"\"\"\n","                    INSERT INTO teams (team_name, city, state, stadium, established_year, colors, mascot, website)\n","                    VALUES (:team_name, :city, :state, :stadium, :established_year, :colors, :mascot, :website)\n","                    ON DUPLICATE KEY UPDATE\n","                    city = VALUES(city),\n","                    state = VALUES(state),\n","                    stadium = VALUES(stadium),\n","                    established_year = VALUES(established_year),\n","                    colors = VALUES(colors),\n","                    mascot = VALUES(mascot),\n","                    website = VALUES(website)\n","                \"\"\")\n","                connection.execute(sql_statement, {\n","                    'team_name': team_name,\n","                    'city': city,\n","                    'state': state,\n","                    'stadium': stadium,\n","                    'established_year': established_year,\n","                    'colors': colors,\n","                    'mascot': mascot,\n","                    'website': website\n","                })\n","            connection.commit()\n","            print(f\"Successfully pushed/updated {len(df)} team records in 'teams' table.\")\n","    except Exception as e:\n","        print(f\"Error pushing teams to SQL: {e}\")\n"]},{"cell_type":"markdown","metadata":{"id":"g-cFILupa2-d"},"source":["### Task 2.6: Implement `push_conferences_to_sql`\n","\n","Create a Python function named `push_conferences_to_sql` that takes a Pandas DataFrame (`conferences_df`) and a SQLAlchemy engine (`sql_engine`) as input. This function should:\n","1.  **Expect `team_id` to already be present in `conferences_df`.**\n","2.  Push the DataFrame to the `conferences` table in the MySQL database.\n","3.  Handle potential duplicates by only inserting new conference records or updating existing ones if the conference/division changed for a team."]},{"cell_type":"markdown","metadata":{"id":"BpzWiWBUa2-d"},"source":["##### Hint\n","<details>\n","<summary>Click to reveal hint</summary>\n","\n","**Step 1: Prepare DataFrame**\n","* The input `conferences_df` should already have `team_id`, `conference`, and `division` columns. Ensure `team_id` is of integer type and handle any `NaN` values if necessary.\n","\n","**Step 2: Implement Upsert Logic (Update or Insert)**\n","* Iterate through each row of your `conferences_df`.\n","* For each row:\n","    * Construct an `INSERT` statement with `ON DUPLICATE KEY UPDATE` clauses for `conference` and `division`. This will insert a new record if `team_id` doesn't exist, or update the `conference` and `division` if `team_id` already exists.\n","    * Execute the SQL statement using `connection.execute(text(...))`.\n","* Remember to `connection.commit()` after the loop to save all changes.\n","</details>"]},{"cell_type":"markdown","metadata":{"id":"tnSXZHVpa2-d"},"source":["##### Solution"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Me9v1Bb9a2-d"},"outputs":[],"source":["def push_conferences_to_sql(conferences_df, sql_engine):\n","    \"\"\"\n","    Pushes the NFL conferences DataFrame to the 'conferences' table.\n","    Updates existing records if conference/division data is unchanged, otherwise inserts new records.\n","    \"\"\"\n","    try:\n","        final_conferences_df = conferences_df.copy()\n","        final_conferences_df.dropna(subset=['team_id'], inplace=True)\n","        final_conferences_df['team_id'] = final_conferences_df['team_id'].astype(int)\n","\n","        if not final_conferences_df.empty:\n","            with sql_engine.connect() as connection:\n","                # Implement Upsert Logic (Update or Insert)\n","                for index, row in final_conferences_df.iterrows():\n","                    team_id = row['team_id']\n","                    conference = row['conference']\n","                    division = row['division']\n","\n","                    sql_statement = text(\"\"\"\n","                        INSERT INTO conferences (team_id, conference, division)\n","                        VALUES (:team_id, :conference, :division)\n","                        ON DUPLICATE KEY UPDATE\n","                        conference = VALUES(conference),\n","                        division = VALUES(division)\n","                    \"\"\")\n","                    connection.execute(sql_statement, {\n","                        'team_id': team_id,\n","                        'conference': conference,\n","                        'division': division\n","                    })\n","                connection.commit()\n","                print(f\"Successfully pushed/updated {len(final_conferences_df)} conference records to 'conferences' table.\")\n","        else:\n","            print(\"No conference data to push (or no matching teams found).\")\n","    except Exception as e:\n","        print(f\"Error pushing conferences to SQL: {e}\")\n"]},{"cell_type":"markdown","metadata":{"id":"lUY8fOXra2-d"},"source":["### Task 2.7: Implement `push_personnel_to_sql`\n","\n","Create a Python function named `push_personnel_to_sql` that takes a Pandas DataFrame (`personnel_df`) and a SQLAlchemy engine (`sql_engine`) as input. This function should:\n","1.  Retrieve `team_id` and `team_name` from the `teams` table in the database.\n","2.  Merge the `personnel_df` with the retrieved team information to add the `team_id`.\n","3.  Push the DataFrame to the `personnel` table in the MySQL database.\n","4.  Handle potential duplicates by inserting new records only if the personnel data for a team has changed for a given `extraction_date`."]},{"cell_type":"markdown","metadata":{"id":"gYnEMfp_a2-d"},"source":["##### Hint\n","<details>\n","<summary>Click to reveal hint</summary>\n","\n","Here's a step-by-step guide to help you implement `push_personnel_to_sql`:\n","\n","**Step 1: Retrieve `team_id`s from the Database**\n","* Use `pd.read_sql('SELECT team_id, team_name FROM teams', sql_engine)` to get the necessary team IDs.\n","* Merge this DataFrame with your input `personnel_df` on `team_name` to associate each personnel record with its corresponding `team_id`.\n","\n","**Step 2: Implement Intelligent Upsert Logic**\n","* Iterate through each row of the merged `personnel_df`.\n","* For each team, first check if a personnel record for that `team_id` already exists in the `personnel` table.\n","    * If a record exists, compare the current personnel data (owner, president, head coach, general manager) with the latest existing record for that team.\n","    * If *any* of the personnel fields have changed then insert a *new* record into the `personnel` table. This preserves historical changes.\n","    * If no record exists, simply insert the new record.\n","* Use `connection.execute(text(...))` for both the `SELECT` (to check for existing data) and `INSERT` statements.\n","\n","**Step 3: Commit Changes**\n","* After the loop, `connection.commit()` to save all changes.\n","</details>"]},{"cell_type":"markdown","metadata":{"id":"3dthRqpRa2-d"},"source":["##### Solution"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ScuH_LUka2-e"},"outputs":[],"source":["def push_personnel_to_sql(personnel_df, sql_engine):\n","    \"\"\"\n","    Pushes the personnel DataFrame to the 'personnel' table.\n","    Retrieves team_ids from the 'teams' table to establish foreign key links.\n","    Inserts new records only if personnel data for a team has changed for a given extraction_date.\n","    \"\"\"\n","    if personnel_df.empty:\n","        print(\"No personnel data to push to 'personnel' table.\")\n","        return\n","\n","    try:\n","        teams_from_db = pd.read_sql('SELECT team_id, team_name FROM teams', sql_engine)\n","        personnel_df_with_ids = pd.merge(personnel_df,\n","                                         teams_from_db,\n","                                         left_on='team_name',\n","                                         right_on='team_name',\n","                                         how='inner')\n","\n","        personnel_df_with_ids.dropna(subset=['team_id'], inplace=True)\n","        personnel_df_with_ids['team_id'] = personnel_df_with_ids['team_id'].astype(int)\n","\n","        if not personnel_df_with_ids.empty:\n","            with sql_engine.connect() as connection:\n","                for index, row in personnel_df_with_ids.iterrows():\n","                    team_id = row['team_id']\n","                    owner = row.get('owner')\n","                    president = row.get('president')\n","                    head_coach = row.get('head_coach')\n","                    general_manager = row.get('general_manager')\n","                    extraction_date = row['extraction_date']\n","\n","                    existing_personnel_query = text(\"\"\"\n","                        SELECT owner, president, head_coach, general_manager, extraction_date\n","                        FROM personnel\n","                        WHERE team_id = :team_id\n","                        ORDER BY extraction_date DESC LIMIT 1\n","                    \"\"\")\n","                    result = connection.execute(existing_personnel_query, {'team_id': team_id}).fetchone()\n","\n","                    should_insert = False\n","                    if result:\n","                        existing_owner, existing_president, existing_head_coach, existing_general_manager, existing_date = result\n","\n","                        existing_date_str = existing_date.strftime('%Y-%m-%d') if existing_date else None\n","\n","                        if (owner != existing_owner or\n","                            president != existing_president or\n","                            head_coach != existing_head_coach or\n","                            general_manager != existing_general_manager):\n","                            should_insert = True\n","                    else:\n","                        should_insert = True # No existing record, so insert\n","\n","                    if should_insert:\n","                        insert_statement = text(\"\"\"\n","                            INSERT INTO personnel (team_id, owner, president, head_coach, general_manager, extraction_date)\n","                            VALUES (:team_id, :owner, :president, :head_coach, :general_manager, :extraction_date)\n","                        \"\"\")\n","                        connection.execute(insert_statement, {\n","                            'team_id': team_id,\n","                            'owner': owner,\n","                            'president': president,\n","                            'head_coach': head_coach,\n","                            'general_manager': general_manager,\n","                            'extraction_date': extraction_date\n","                        })\n","                connection.commit()\n","                print(f\"Successfully processed personnel data for {len(personnel_df_with_ids)} teams.\")\n","        else:\n","            print(\"No personnel data to push (or no matching teams found).\")\n","    except Exception as e:\n","        print(f\"Error pushing personnel to SQL: {e}\")\n"]},{"cell_type":"markdown","metadata":{"id":"feDc2pa3a2-e"},"source":["### Task 2.8: Implement `push_championships_to_sql`\n","\n","Create a Python function named `push_championships_to_sql` that takes a Pandas DataFrame (`championships_df`) and a SQLAlchemy engine (`sql_engine`) as input. This function should:\n","1.  Retrieve `team_id` and `team_name` from the `teams` table in the database.\n","2.  Merge the `championships_df` with the retrieved team information to add the `team_id`.\n","3.  Push the DataFrame to the `championships` table in the MySQL database.\n","4.  Handle potential duplicates by updating existing records for a given `team_id` if the championship counts have changed, or insert a new one if no record exists."]},{"cell_type":"markdown","metadata":{"id":"znxdzubja2-e"},"source":["##### Hint\n","<details>\n","<summary>Click to reveal hint</summary>\n","\n","Here's a step-by-step guide to help you implement `push_championships_to_sql`:\n","\n","**Step 1: Retrieve `team_id`s from the Database**\n","* Use `pd.read_sql('SELECT team_id, team_name FROM teams', sql_engine)` to get the necessary team IDs.\n","* Merge this DataFrame with your input `championships_df` on `team_name` to associate each championship record with its corresponding `team_id`.\n","\n","**Step 2: Implement Upsert Logic (Update or Insert)**\n","* Iterate through each row of the merged `championships_df`.\n","* For each row, construct an `INSERT` statement with `ON DUPLICATE KEY UPDATE` clauses for `league_championships`, `conference_championships`, and `division_championships`.\n","* This will insert a new record if `team_id` doesn't exist, or update the championship counts if `team_id` already exists.\n","* Execute this SQL statement using `connection.execute(text(...))` within a database connection.\n","\n","**Step 3: Commit Changes**\n","* After iterating through all rows, commit the changes to the database using `connection.commit()`.\n","</details>"]},{"cell_type":"markdown","metadata":{"id":"mjpyp1Bxa2-e"},"source":["##### Solution"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"htq3vgLka2-e"},"outputs":[],"source":["def push_championships_to_sql(championships_df, sql_engine):\n","    \"\"\"\n","    Pushes the championship DataFrame to the 'championships' table.\n","    Retrieves team_ids from the 'teams' table to establish foreign key links.\n","    Updates existing records for a given team_id or inserts new ones.\n","    \"\"\"\n","    if championships_df.empty:\n","        print(\"No championship data to push to 'championships' table.\")\n","        return\n","\n","    try:\n","        teams_from_db = pd.read_sql('SELECT team_id, team_name FROM teams', sql_engine)\n","        championships_df_with_ids = pd.merge(championships_df,\n","                                             teams_from_db,\n","                                             left_on='team_name',\n","                                             right_on='team_name',\n","                                             how='inner')\n","\n","        championships_df_with_ids.dropna(subset=['team_id'], inplace=True)\n","        championships_df_with_ids['team_id'] = championships_df_with_ids['team_id'].astype(int)\n","\n","        if not championships_df_with_ids.empty:\n","            with sql_engine.connect() as connection:\n","                for index, row in championships_df_with_ids.iterrows():\n","                    team_id = row['team_id']\n","                    league_championships = row.get('league_championships')\n","                    conference_championships = row.get('conference_championships')\n","                    division_championships = row.get('division_championships')\n","\n","                    sql_statement = text(\"\"\"\n","                        INSERT INTO championships (team_id, league_championships, conference_championships, division_championships)\n","                        VALUES (:team_id, :league_championships, :conference_championships, :division_championships)\n","                        ON DUPLICATE KEY UPDATE\n","                        league_championships = VALUES(league_championships),\n","                        conference_championships = VALUES(conference_championships),\n","                        division_championships = VALUES(division_championships)\n","                    \"\"\")\n","                    connection.execute(sql_statement, {\n","                        'team_id': team_id,\n","                        'league_championships': league_championships,\n","                        'conference_championships': conference_championships,\n","                        'division_championships': division_championships\n","                    })\n","                connection.commit()\n","                print(f\"Successfully pushed/updated {len(championships_df_with_ids)} championship records to 'championships' table.\")\n","        else:\n","            print(\"No championship data to push (or no matching teams found).\")\n","    except Exception as e:\n","        print(f\"Error pushing championships to SQL: {e}\")\n"]},{"cell_type":"markdown","metadata":{"id":"uhJuVidaa2-f"},"source":["## Part 3: Implementing Main Workflow Functions\n","\n","Now, you will implement the main functions that orchestrate the entire data pipeline: scraping data for teams, personnel, and championships, and then storing them in the database. These functions will leverage the helper functions you just created."]},{"cell_type":"markdown","metadata":{"id":"-Lh9Y2rMa2-f"},"source":["### Task 3.1: Implement `scrap_and_store_teams_and_conferences`\n","\n","Create a Python function named `scrap_and_store_teams_and_conferences` that takes `sql_engine` as input. This function should:\n","1.  Call `get_nfl_teams_from_wiki` to scrape the initial team, conference, and division data.\n","2.  Extract the unique team names from the scraped data to prepare for the `teams` table.\n","3.  Push these unique team names to the `teams` table.\n","4.  After the `teams` table is populated (and `team_id`s are generated), merge the original scraped data with the `team_id`s from the database.\n","5.  Prepare a DataFrame for the `conferences` table with `conference`, `division`, and `team_id`.\n","6.  Push this `conferences` DataFrame to the `conferences` table using `push_conferences_to_sql`."]},{"cell_type":"markdown","metadata":{"id":"STm3yHWTa2-f"},"source":["##### Hint\n","<details>\n","<summary>Click to reveal hint</summary>\n","\n","Here's a step-by-step guide to help you implement `scrap_and_store_teams_and_conferences`:\n","\n","**Step 1: Scrape Initial NFL Teams Data**\n","* Call `get_nfl_teams_from_wiki()` to get the DataFrame containing 'conference', 'division', and 'team_name'. Let's call this `initial_nfl_data_df`.\n","\n","**Step 2: Prepare and Push to `teams` table**\n","* Create a new DataFrame for the `teams` table, containing only the unique `team_name` values from `initial_nfl_data_df`.\n","* Call `push_teams_to_sql(teams_to_push_df, sql_engine)`.\n","\n","**Step 3: Retrieve `team_id`s and Prepare `conferences` table data**\n","* After pushing to `teams`, retrieve the `team_id`s and `team_name`s from the `teams` table in the database using `pd.read_sql`.\n","* Merge `initial_nfl_data_df` with this `teams_from_db_df` on the team name to associate `team_id`s with conference and division data.\n","* Create a `conferences_df` with 'conference', 'division', and 'team_id' columns. Ensure the column names match the SQL schema.\n","\n","**Step 4: Push to `conferences` table**\n","* Call `push_conferences_to_sql(conferences_df, sql_engine)`.\n","</details>"]},{"cell_type":"markdown","metadata":{"id":"RMeCDu6Xa2-p"},"source":["##### Solution"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W00Gl0HCa2-q"},"outputs":[],"source":["def scrap_and_store_teams_and_conferences(sql_engine):\n","    \"\"\"\n","    Scrapes initial NFL team data (including conference and division),\n","    stores unique teams in the 'teams' table, and then populates the 'conferences' table.\n","    \"\"\"\n","    print(\"Scraping initial NFL teams and conference data from Wikipedia...\")\n","    initial_nfl_data_df = get_nfl_teams_from_wiki()\n","\n","    if initial_nfl_data_df.empty:\n","        print(\"No initial NFL team data scraped. Skipping database operations for teams and conferences.\")\n","        return\n","\n","    all_teams_static_data = []\n","    # Filter out any None or empty team_name values before iterating\n","    valid_team_names = [name for name in initial_nfl_data_df['team_name'].unique() if name and str(name).strip() != '']\n","\n","    for team_name in valid_team_names:\n","        static_info = get_static_team_data(team_name)\n","        all_teams_static_data.append(static_info)\n","    static_teams_df = pd.DataFrame(all_teams_static_data)\n","\n","    # Ensure 'team_name' column in static_teams_df is also clean and not empty\n","    static_teams_df = static_teams_df.dropna(subset=['team_name'])\n","    static_teams_df = static_teams_df[static_teams_df['team_name'].astype(str).str.strip() != ''].copy()\n","\n","    push_teams_to_sql(teams_to_push_df, sql_engine)\n","\n","    teams_from_db_df = pd.read_sql('SELECT team_id, team_name FROM teams', sql_engine)\n","\n","    # Merge initial_nfl_data_df with teams_from_db_df to get team_id before passing to push_conferences_to_sql\n","    conferences_df = pd.merge(initial_nfl_data_df,\n","                              teams_from_db_df,\n","                              on='team_name',\n","                              how='inner')\n","    # Select only the columns relevant for the conferences table AFTER the merge\n","    conferences_df = conferences_df[['conference', 'division', 'team_id']].copy()\n","\n","    push_conferences_to_sql(conferences_df, sql_engine)\n",""]},{"cell_type":"markdown","metadata":{"id":"EdU6Fgoha2-r"},"source":["### Task 3.2: Implement `scrap_and_store_current_personnel`\n","\n","Create a Python function named `scrap_and_store_current_personnel` that takes `sql_engine` as input. This function should:\n","1.  Use the provided SQLAlchemy engine.\n","2.  Read the list of `team_name`s directly from the `teams` table in your database.\n","3.  For each team, call `get_personnel_data` to scrape its current personnel information.\n","4.  Collect all scraped data into a Pandas DataFrame.\n","5.  Call `push_personnel_to_sql` to store the DataFrame in the `personnel` table.\n","\n","**Note on Data Handling:** This function is designed to intelligently update or append new personnel records, preserving historical data only when actual personnel changes occur."]},{"cell_type":"markdown","metadata":{"id":"ElrqAH3oa2-r"},"source":["##### Hint\n","<details>\n","<summary>Click to reveal hint</summary>\n","\n","Here's a step-by-step guide to help you implement `scrap_and_store_current_personnel`:\n","\n","**Step 1: Get Team Names from Database**\n","* Use `pd.read_sql('SELECT team_name FROM teams', sql_engine)` to get the list of team names.\n","\n","**Step 2: Initialize Data Collection List**\n","* Create an empty list (e.g., `personnel_data_list = []`) to store the dictionaries returned by `get_personnel_data`.\n","\n","**Step 3: Loop and Scrape Personnel Data**\n","* Iterate through each `team_name` in the retrieved list.\n","* Inside the loop, call `get_personnel_data(team_name)` to fetch the current personnel information for that team.\n","* Append the returned dictionary to your `personnel_data_list`.\n","\n","**Step 4: Create DataFrame**\n","* After the loop, convert the `personnel_data_list` into a Pandas DataFrame (e.g., `nfl_personnel_df = pd.DataFrame(personnel_data_list)`).\n","\n","**Step 5: Push to SQL**\n","* Call your `push_personnel_to_sql` helper function, passing the newly created `nfl_personnel_df` and the `sql_engine`.\n","</details>"]},{"cell_type":"markdown","metadata":{"id":"EdG1Y8MTa2-r"},"source":["##### Solution"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PElDl-rZa2-s"},"outputs":[],"source":["def scrap_and_store_current_personnel(sql_engine):\n","    \"\"\"\n","    Scrapes current personnel data for all teams from Wikipedia and stores it in the 'personnel' table.\n","    Teams are retrieved from the database to ensure correct team_ids.\n","    \"\"\"\n","    try:\n","        # Read team names directly from the 'teams' table\n","        teams_from_db_df = pd.read_sql('SELECT team_name FROM teams', sql_engine)\n","        list_of_teams = teams_from_db_df['team_name'].tolist()\n","\n","        if not list_of_teams:\n","            print(\"No teams found in the database. Skipping personnel data scraping.\")\n","            return\n","\n","        personnel_data_list = []\n","        for team_name in list_of_teams:\n","            personnel_data = get_personnel_data(team_name)\n","            personnel_data_list.append(personnel_data)\n","\n","        nfl_personnel_df = pd.DataFrame(personnel_data_list)\n","        push_personnel_to_sql(nfl_personnel_df, sql_engine)\n","        print(\"Current NFL Personnel data scraped and stored successfully!\")\n","    except Exception as e:\n","        print(f\"Error in scraping and storing personnel data: {e}\")\n"]},{"cell_type":"markdown","metadata":{"id":"WT4pQMDma2-s"},"source":["### Task 3.3: Implement `scrap_and_store_current_championships`\n","\n","Create a Python function named `scrap_and_store_current_championships` that takes `sql_engine` as input. This function should:\n","1.  Use the provided SQLAlchemy engine.\n","2.  Read the list of `team_name`s directly from the `teams` table in your database.\n","3.  For each team, call `get_championship_data` to scrape its current championship information.\n","4.  Collect all scraped data into a Pandas DataFrame.\n","5.  Call `push_championships_to_sql` to store the DataFrame in the `championships` table.\n","\n","**Note on Data Handling:** This function is designed to **update** existing records for a given `team_id` if the championship counts have changed, or insert a new one if no record exists. This ensures that championship totals are always current, reflecting the latest information from Wikipedia, and historical `extraction_year` data is NOT maintained in this table."]},{"cell_type":"markdown","metadata":{"id":"xE1Ntg2qa2-s"},"source":["##### Hint\n","<details>\n","<summary>Click to reveal hint</summary>\n","\n","Here's a step-by-step guide to help you implement `scrap_and_store_current_championships`:\n","\n","**Step 1: Get Team Names from Database**\n","* Use `pd.read_sql('SELECT team_name FROM teams', sql_engine)` to get the list of team names.\n","\n","**Step 2: Initialize Data Collection List**\n","* Create an empty list (e.g., `championship_data_list = []`) to store the dictionaries returned by `get_championship_data`.\n","\n","**Step 3: Loop and Scrape Championship Data**\n","* Iterate through each `team_name` in the retrieved list.\n","* Inside the loop, call `get_championship_data(team_name)` to fetch the current championship information for that team.\n","* Append the returned dictionary to your `championship_data_list`.\n","\n","**Step 4: Create DataFrame**\n","* After the loop, convert the `championship_data_list` into a Pandas DataFrame (e.g., `nfl_championships_df = pd.DataFrame(championship_data_list)`).\n","\n","**Step 5: Push to SQL**\n","* Call your `push_championships_to_sql` helper function, passing the newly created `nfl_championships_df` and the `sql_engine`.\n","</details>"]},{"cell_type":"markdown","metadata":{"id":"F1pQbrdEa2-s"},"source":["##### Solution"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e5dUS3Bga2-t"},"outputs":[],"source":["def scrap_and_store_current_championships(sql_engine):\n","    \"\"\"\n","    Scrapes current championship data for all teams from Wikipedia and stores it in the 'championships' table.\n","    Teams are retrieved from the database to ensure correct team_ids.\n","    \"\"\"\n","    try:\n","        # Read team names directly from the 'teams' table\n","        teams_from_db_df = pd.read_sql('SELECT team_name FROM teams', sql_engine)\n","        list_of_teams = teams_from_db_df['team_name'].tolist()\n","\n","        if not list_of_teams:\n","            print(\"No teams found in the database. Skipping championship data scraping.\")\n","            return\n","\n","        championship_data_list = []\n","        for team_name in list_of_teams:\n","            championship_data = get_championship_data(team_name)\n","            championship_data_list.append(championship_data)\n","\n","        nfl_championships_df = pd.DataFrame(championship_data_list)\n","\n","        push_championships_to_sql(nfl_championships_df, sql_engine)\n","        print(\"Current NFL Championships data scraped and stored successfully!\")\n","    except Exception as e:\n","        print(f\"Error in scraping and storing championship data: {e}\")\n"]},{"cell_type":"markdown","metadata":{"id":"2qZHtjXva2-t"},"source":["## Part 4: Executing the Data Pipeline\n","\n","Now that all helper and main workflow functions are defined, it's time to execute the data pipeline. You will set up the database connection, and then call the main scraping and storage functions in sequence."]},{"cell_type":"markdown","metadata":{"id":"pP2z-b3ba2-t"},"source":["### Task 4.1: Establish MySQL Connection and Create Tables\n","\n","**Your Task:** Establish a connection to your MySQL database using `sqlalchemy`. Define the connection parameters (schema, host, user, password, port) and create the database engine. Then, ensure all necessary tables (`teams`, `conferences`, `personnel`, `championships`) exist in your database. You can choose one of the two methods below to create your tables.\n","\n","#### Option A: Manual MySQL Client Setup\n","\n","If you prefer to manage your database schema directly, you can use a MySQL client (like MySQL Workbench or the command line) to create the `sql_workshop` database and the necessary tables. After running the SQL commands in your client, you will then define the SQLAlchemy engine in Python to connect to this pre-existing database.\n","\n","##### Your Task (Part 1): Manually create the database and all necessary tables in your MySQL client.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x18Ypptoa2-t"},"outputs":[],"source":["# No Python code needed here for manual setup. Execute SQL in your MySQL client."]},{"cell_type":"markdown","metadata":{"id":"bnq90zJSa2-u"},"source":["##### Hint\n","<details>\n","<summary>Click to reveal hint</summary>\n","\n","Connect to your MySQL server using your preferred client. Then, copy and paste the SQL commands from the solution section below and execute them. Ensure there are no errors during execution.\n","\n","After successful execution, you can verify the tables exist by running `SHOW TABLES;` within the `sql_workshop` database in your MySQL client.\n","</details>"]},{"cell_type":"markdown","metadata":{"id":"pRGxGwqma2-v"},"source":["##### Solution (for Option A - Part 1: SQL Commands)\n","<details>\n","<summary>Click to reveal solution</summary>\n","\n","Run the following SQL commands in your MySQL client:\n","```sql\n","CREATE DATABASE IF NOT EXISTS sql_workshop;\n","USE sql_workshop;\n","\n","CREATE TABLE IF NOT EXISTS teams (\n","    team_id INT PRIMARY KEY AUTO_INCREMENT,\n","    team_name VARCHAR(255) NOT NULL UNIQUE,\n","    city VARCHAR(255),\n","    state VARCHAR(255),\n","    stadium VARCHAR(255),\n","    established_year INT,\n","    colors TEXT,\n","    mascot VARCHAR(255),\n","    website VARCHAR(255)\n",");\n","\n","CREATE TABLE IF NOT EXISTS conferences (\n","    team_id INT PRIMARY KEY,\n","    conference VARCHAR(255),\n","    division VARCHAR(255),\n","    FOREIGN KEY (team_id) REFERENCES teams(team_id)\n",");\n","\n","CREATE TABLE IF NOT EXISTS personnel (\n","    personnel_id INT PRIMARY KEY AUTO_INCREMENT,\n","    team_id INT,\n","    owner TEXT,\n","    president TEXT,\n","    head_coach TEXT,\n","    general_manager TEXT,\n","    extraction_date DATE,\n","    FOREIGN KEY (team_id) REFERENCES teams(team_id)\n",");\n","\n","CREATE TABLE IF NOT EXISTS championships (\n","    team_id INT PRIMARY KEY,\n","    league_championships INT,\n","    conference_championships INT,\n","    division_championships INT,\n","    FOREIGN KEY (team_id) REFERENCES teams(team_id)\n",");\n","```\n","After executing the above SQL, your database schema will be ready. Proceed to the next part to define the Python engine.\n","</details>"]},{"cell_type":"markdown","metadata":{"id":"Jr9nl9gCa2-v"},"source":["##### Your Task (Part 2): Now that the database and tables are manually created, define the connection parameters and establish the SQLAlchemy engine in Python to connect to the `sql_workshop` database.\n"]},{"cell_type":"markdown","metadata":{"id":"0AX89oj1a2-v"},"source":["##### Hint\n","<details>\n","<summary>Click to reveal hint</summary>\n","\n","Define the `schema`, `host`, `user`, `password` (from `keys.MySQL_pass`), and `port` variables.\n","Construct the database connection string using an f-string.\n","Use `create_engine()` from `sqlalchemy`.\n","Include a `try-except` block to handle `OperationalError` for connection issues.\n","You can test the connection with a simple query like `connection.execute(text(\"SELECT 1\"))`.\n","</details>"]},{"cell_type":"markdown","metadata":{"id":"pCecgIlza2-w"},"source":["##### Solution"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RIOWylpFa2-x"},"outputs":[],"source":["schema = \"sql_workshop\"\n","host = \"127.0.0.1\"\n","user = \"root\"\n","password = keys.MySQL_pass\n","port = 3306\n","\n","db_connection_string = f'mysql+pymysql://{user}:{password}@{host}:{port}/{schema}'\n","\n","engine = None\n","try:\n","    engine = create_engine(db_connection_string)\n","    with engine.connect() as connection:\n","        connection.execute(text(\"SELECT 1\"))\n","    print(\"SQLAlchemy engine for 'sql_workshop' created and connected successfully.\")\n","\n","except OperationalError as e:\n","    print(f\"Error connecting to MySQL database: {e}\")\n","    print(\"Please ensure your MySQL server is running, 'sql_workshop' database exists, and credentials are correct.\")\n","except Exception as e:\n","    print(f\"An unexpected error occurred during engine creation or connection test: {e}\")"]},{"cell_type":"markdown","metadata":{"id":"whORY_ara2-x"},"source":["#### Option B: Python `connection.execute` Setup\n","\n","If you prefer to automate the database and table creation process using Python, you can define and call a function that uses SQLAlchemy's `connection.execute` to run the `CREATE DATABASE IF NOT EXISTS` and `CREATE TABLE IF NOT EXISTS` commands. This approach ensures your schema is set up programmatically.\n","\n","**Your Task:** Define the connection parameters, establish the SQLAlchemy engine (including creating the database if it doesn't exist), and then define and call the `create_nfl_tables_mysql` function to create the necessary tables in your database using Python.\n"]},{"cell_type":"markdown","metadata":{"id":"wfyNn55na2-x"},"source":["##### Hint\n","<details>\n","<summary>Click to reveal hint</summary>\n","\n","Define the `schema`, `host`, `user`, `password`, and `port` variables. Then, create two connection strings: one for the server (`server_connection_string`) and one for the specific database (`db_connection_string`).\n","\n","Use a `try-except` block to first connect to the server (using `server_connection_string`) and execute `CREATE DATABASE IF NOT EXISTS {schema};`. After that, create the main `engine` using `db_connection_string`.\n","\n","Define a function, for example, `create_nfl_tables_mysql(engine)`, that encapsulates the SQL `CREATE TABLE IF NOT EXISTS` statements. Inside this function, use a `with engine.connect() as connection:` block to execute each SQL command using `connection.execute(text(sql_command))` and `connection.commit()`.\n","\n","Finally, call the `create_nfl_tables_mysql(engine)` function.\n","\n","Ensure `team_id` is set as the `PRIMARY KEY` for the `championships` and `conferences` tables.\n","</details>"]},{"cell_type":"markdown","metadata":{"id":"XXrpIGZSa2-y"},"source":["##### Solution (for Option B)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LWFNS0L-a2-y"},"outputs":[],"source":["schema = \"sql_workshop\"\n","host = \"127.0.0.1\"\n","user = \"root\"\n","password = keys.MySQL_pass\n","port = 3306\n","\n","server_connection_string = f'mysql+pymysql://{user}:{password}@{host}:{port}/'\n","db_connection_string = f'mysql+pymysql://{user}:{password}@{host}:{port}/{schema}'\n","\n","engine = None\n","\n","try:\n","    server_engine = create_engine(server_connection_string)\n","    with server_engine.connect() as connection:\n","        connection.execute(text(f\"CREATE DATABASE IF NOT EXISTS {schema};\"))\n","        connection.commit()\n","    print(f\"Database '{schema}' created or already exists.\")\n","\n","    engine = create_engine(db_connection_string)\n","\n","except OperationalError as e:\n","    print(f\"Error connecting to MySQL server or database: {e}\")\n","    print(\"Please ensure your MySQL server is running and connection details (host, port, user, password) are correct.\")\n","except Exception as e:\n","    print(f\"An unexpected error occurred during database creation or engine creation: {e}\")\n","\n","def create_nfl_tables_mysql(engine):\n","    \"\"\"\n","    Ensures the 'teams', 'conferences', and 'championships' tables exist in the MySQL database.\n","    \"\"\"\n","    if engine is None:\n","        print(\"Database engine not initialized. Cannot create tables.\")\n","        return\n","\n","    try:\n","        with engine.connect() as connection:\n","            sql_commands = [\n","                \"\"\"\n","                CREATE TABLE IF NOT EXISTS teams (\n","                    team_id INT PRIMARY KEY AUTO_INCREMENT,\n","                    team_name VARCHAR(255) NOT NULL UNIQUE,\n","                    city VARCHAR(255),\n","                    state VARCHAR(255),\n","                    stadium VARCHAR(255),\n","                    established_year INT,\n","                    colors TEXT,\n","                    mascot VARCHAR(255),\n","                    website VARCHAR(255)\n","                );\n","                \"\"\",\n","                \"\"\"\n","                CREATE TABLE IF NOT EXISTS conferences (\n","                    team_id INT PRIMARY KEY,\n","                    conference VARCHAR(255),\n","                    division VARCHAR(255),\n","                    FOREIGN KEY (team_id) REFERENCES teams(team_id)\n","                );\n","                \"\"\",\n","                \"\"\"\n","                CREATE TABLE IF NOT EXISTS personnel (\n","                    personnel_id INT PRIMARY KEY AUTO_INCREMENT,\n","                    team_id INT,\n","                    owner TEXT,\n","                    president TEXT,\n","                    head_coach TEXT,\n","                    general_manager TEXT,\n","                    extraction_date DATE,\n","                    FOREIGN KEY (team_id) REFERENCES teams(team_id)\n","                );\n","                \"\"\",\n","                \"\"\"\n","                CREATE TABLE IF NOT EXISTS championships (\n","                    team_id INT PRIMARY KEY,\n","                    league_championships INT,\n","                    conference_championships INT,\n","                    division_championships INT,\n","                    FOREIGN KEY (team_id) REFERENCES teams(team_id)\n","                );\n","                \"\"\"\n","            ]\n","\n","            for cmd in sql_commands:\n","                connection.execute(text(cmd))\n","                connection.commit()\n","            print(\"All NFL tables prepared (or confirmed) using Python.\")\n","    except OperationalError as e:\n","        print(f\"Error during table creation: {e}\")\n","        print(\"Please ensure you have the necessary permissions for the database and tables.\")\n","    except ProgrammingError as e:\n","        print(f\"SQL syntax error during table creation: {e}\")\n","        print(\"Please review the SQL schema for any syntax issues.\")\n","    except Exception as e:\n","        print(f\"An unexpected error occurred during table creation: {e}\")\n","\n","if engine is not None:\n","    create_nfl_tables_mysql(engine)\n","else:\n","    print(\"Skipping table creation due to prior database connection error.\")"]},{"cell_type":"markdown","metadata":{"id":"TWrineNta2-y"},"source":["### Task 4.2: Execute `scrap_and_store_teams_and_conferences`\n","\n","Call the `scrap_and_store_teams_and_conferences` function to begin scraping initial team, conference, and division data and storing it in your database."]},{"cell_type":"markdown","metadata":{"id":"p2gTvdBpa2-z"},"source":["##### Hint\n","<details>\n","<summary>Click to reveal hint</summary>\n","\n","Call the function you defined in Task 3.1, passing the `sql_engine` (which is the `engine` variable from Task 4.1).\n","\n","```python\n","scrap_and_store_teams_and_conferences(engine)\n","```\n","</details>"]},{"cell_type":"markdown","metadata":{"id":"S584JH0Ga2-z"},"source":["##### Solution"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fz1socLQa2-z"},"outputs":[],"source":["print(\"\\n--- Starting Initial Team & Conference Data Scraping and Storage ---\")\n","if 'engine' in locals() and engine is not None:\n","    scrap_and_store_teams_and_conferences(engine)\n","else:\n","    print(\"Database engine not initialized. Skipping initial team and conference data scraping.\")\n","print(\"--- Initial Team & Conference Data Scraping and Storage Complete! ---\")"]},{"cell_type":"markdown","metadata":{"id":"WEvtLxzNa2-0"},"source":["### Task 4.3: Execute `scrap_and_store_current_personnel`\n","\n","Now, call the `scrap_and_store_current_personnel` function to scrape and store the latest personnel data for all teams. This function will retrieve the list of team names directly from the database to ensure data consistency.\n","\n","**Note on Data Handling:** The `push_personnel_to_sql` function is designed to intelligently update or append new personnel records, preserving historical data only when actual personnel changes occur."]},{"cell_type":"markdown","metadata":{"id":"n56IdR-La2-0"},"source":["##### Hint\n","<details>\n","<summary>Click to reveal hint</summary>\n","\n","Call the function you defined in Task 3.2, passing the `sql_engine`.\n","The function will internally retrieve the list of teams from the database.\n","\n","```python\n","scrap_and_store_current_personnel(engine)\n","```\n","</details>"]},{"cell_type":"markdown","metadata":{"id":"MrFMGlDia2-0"},"source":["##### Solution"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DwzM9kQYa2-0"},"outputs":[],"source":["print(\"\\n--- Starting Personnel Data Scraping and Storage ---\")\n","if 'engine' in locals() and engine is not None:\n","    scrap_and_store_current_personnel(engine)\n","else:\n","    print(\"Database engine not initialized. Skipping personnel data scraping.\")\n","print(\"--- Personnel Data Scraping and Storage Complete! ---\")"]},{"cell_type":"markdown","metadata":{"id":"OZX9Vuf0a2-1"},"source":["### Task 4.4: Execute `scrap_and_store_current_championships`\n","\n","Finally, call the `scrap_and_store_current_championships` function to scrape and store the latest championship data. This will complete the data acquisition and storage pipeline.\n","\n","**Note on Data Handling:** This function is designed to **update** existing records for a given `team_id` if the championship counts have changed, or insert a new one if no record exists. This ensures that championship totals are always current, reflecting the latest information from Wikipedia, and historical `extraction_year` data is NOT maintained in this table."]},{"cell_type":"markdown","metadata":{"id":"K1ZZEN2Ha2-1"},"source":["##### Hint\n","<details>\n","<summary>Click to reveal hint</summary>\n","\n","Call the function you defined in Task 3.3, passing the `sql_engine`.\n","The function will internally retrieve the list of teams from the database.\n","\n","```python\n","scrap_and_store_current_championships(engine)\n","```\n","</details>"]},{"cell_type":"markdown","metadata":{"id":"6QHfIKjLa2-2"},"source":["##### Solution"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9Q0UrJr2a2-2"},"outputs":[],"source":["print(\"\\n--- Starting Championships Data Scraping and Storage ---\")\n","if 'engine' in locals() and engine is not None:\n","    scrap_and_store_current_championships(engine)\n","else:\n","    print(\"Database engine not initialized. Skipping championships data scraping.\")\n","print(\"--- Championships Data Scraping and Storage Complete! ---\")"]},{"cell_type":"markdown","metadata":{"id":"qv3iCtXna2-2"},"source":["## Congratulations!\n","You've successfully completed the NFL Data Science Workshop. You've learned to scrape data from the web, organize it with Pandas, and integrate it into a MySQL database, setting the foundation for further data analysis and insights!"]}],"metadata":{"kernelspec":{"display_name":".venv","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.9"},"colab":{"provenance":[],"collapsed_sections":["XnXnG6Jha2-S","7X5javnna2-V","kdhP8W8Xa2-Y","CZcgK2xea2-a","MVjC0fh2a2-c","tnSXZHVpa2-d","3dthRqpRa2-d","mjpyp1Bxa2-e","RMeCDu6Xa2-p","EdG1Y8MTa2-r","F1pQbrdEa2-s","pCecgIlza2-w","XXrpIGZSa2-y","S584JH0Ga2-z","MrFMGlDia2-0","6QHfIKjLa2-2"]}},"nbformat":4,"nbformat_minor":0}